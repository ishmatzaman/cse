{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj2yr/yzHM7xcGrrmtFjoX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishmatzaman/cse/blob/main/lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsZZEpRUzsCa",
        "outputId": "03e9579c-d2be-4ca1-8fc6-75a59e1801ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(\n",
        "    root=r'/content/drive/MyDrive/CSE366/caltech-101/__MACOSX/caltech-101/101_ObjectCategories/101_ObjectCategories',\n",
        "    transform=transform\n",
        ")\n"
      ],
      "metadata": {
        "id": "jHVnM_p-nYp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_data, val_data, test_data = random_split(dataset, [train_size,\n",
        "val_size, test_size])\n"
      ],
      "metadata": {
        "id": "UhULOT1knYmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "test_loader = DataLoader(test_data, batch_size=32)"
      ],
      "metadata": {
        "id": "pCLLkJMsnYi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torchvision.models import vgg19\n",
        "model = vgg19(pretrained=True)\n",
        "model.classifier[6] = torch.nn.Linear(4096, 101)"
      ],
      "metadata": {
        "id": "-2a9stnTnYgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50\n",
        "model = resnet50(pretrained=True)\n",
        "\n",
        "model.fc = torch.nn.Linear(2048, 101)"
      ],
      "metadata": {
        "id": "pTVNLX-snYdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import efficientnet_b0\n",
        "model = efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(1280, 101)"
      ],
      "metadata": {
        "id": "AubxC4ZYnYYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "7T_Eyp56nYV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the selected device\n",
        "model.to(device)\n",
        "\n",
        "# Example: Move a tensor to the selected device\n",
        "tensor = torch.randn(10).to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "ZvpPYMWinv9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels % 101\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "Wv0a4Mq_nv6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        # Move images and labels to the same device as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)"
      ],
      "metadata": {
        "id": "KIR4w4Pkn3MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install scikit-learn\n",
        "\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "\t'lr': [0.1, 0.01, 0.001],\n",
        "\t'batch_size': [16, 32, 64]\n",
        "}\n",
        "\n",
        "best_params = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\ttrain_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
        "\n",
        "\t# Perform training and validation here\n",
        "\t# For demonstration, let's assume accuracy is calculated and assigned\n",
        "\taccuracy = 0.85  # Replace with actual accuracy calculation\n",
        "\n",
        "\tif accuracy > best_accuracy:\n",
        "\t\tbest_accuracy = accuracy\n",
        "\t\tbest_params = params\n",
        "\n",
        "print(f\"Best Params: {best_params}\")\n"
      ],
      "metadata": {
        "id": "Mh6lKMkTn3JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move images and labels to the same device as the model\n",
        "        images = images.to(device)  # Move images to device\n",
        "        labels = labels.to(device)  # Move labels to device\n",
        "        outputs = model(images)\n",
        "        # Compute test metrics"
      ],
      "metadata": {
        "id": "_APo7CLxn9xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "\tfor images, labels in test_loader:\n",
        "\t\t# Move images and labels to the same device as the model\n",
        "\t\timages = images.to(device)  # Move images to device\n",
        "\t\tlabels = labels.to(device)  # Move labels to device # Added this line\n",
        "\t\toutputs = model(images)\n",
        "\t\t_, preds = torch.max(outputs, 1)\n",
        "\t\ty_pred.extend(preds.cpu().numpy()) # Moved preds to CPU before converting to numpy\n",
        "\t\ty_true.extend(labels.cpu().numpy()) # Moved labels to CPU before converting to numpy\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "ucEM1rXQn9uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = dataset.classes  # Define class_names based on the dataset\n",
        "\n",
        "# Ensure the number of class names matches the number of unique labels in y_true\n",
        "unique_labels = sorted(set(y_true))\n",
        "adjusted_class_names = [class_names[i] for i in unique_labels]\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred, labels=unique_labels, target_names=adjusted_class_names))"
      ],
      "metadata": {
        "id": "nHpQuEbjn9q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def top_k_accuracy(output, target, k=5):\n",
        "\twith torch.no_grad():\n",
        "\t\tmax_k_preds = torch.topk(output, k, dim=1).indices\n",
        "\t\tcorrect = max_k_preds.eq(target.view(-1, 1).expand_as(max_k_preds))\n",
        "\t\treturn correct.any(dim=1).float().mean().item()"
      ],
      "metadata": {
        "id": "_bBbLuDZnv0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "for i, acc in enumerate(per_class_accuracy):\n",
        "\tprint(f\"Class {class_names[i]} Accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "id": "e8W-A1SooDb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "XoKOLCSPoDZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "features = []\n",
        "labels_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move images to the same device as the model\n",
        "        images = images.to(device)  # Move images to device\n",
        "\n",
        "        output = model(images)\n",
        "        features.append(output)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "features = torch.cat(features).cpu().numpy() # Move features to CPU before converting to numpy\n",
        "labels_list = torch.cat(labels_list).cpu().numpy() # Move labels_list to CPU before converting to numpy\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_features = tsne.fit_transform(features)\n",
        "plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=labels_list, cmap='tab10')\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IIU0Hdy_oJw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install grad-cam"
      ],
      "metadata": {
        "id": "hAFhJv9PoJuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Adjust based on the model's architecture\n",
        "# Check model architecture and set target layer\n",
        "if isinstance(model, torch.nn.Sequential):  # VGG19\n",
        "    target_layer = model.features[-1]\n",
        "elif isinstance(model, torch.nn.Module) and hasattr(model, 'layer4'):  # ResNet50\n",
        "    target_layer = model.layer4[-1]\n",
        "elif isinstance(model, torch.nn.Module) and hasattr(model, 'features'):  # EfficientNet-B0\n",
        "    target_layer = model.features[-1]\n",
        "else:\n",
        "    raise ValueError(\"Model architecture not recognized\")\n",
        "\n",
        "# Initialize GradCAM\n",
        "cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "\n",
        "# Generate and display CAMs\n",
        "count = 0\n",
        "for images, labels in test_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    labels = labels % 101  # Ensure labels are within the correct range\n",
        "    targets = [ClassifierOutputTarget(label.item()) for label in labels]\n",
        "    grayscale_cam = cam(input_tensor=images, targets=targets)\n",
        "    for i in range(len(images)):\n",
        "        if count >= 5:\n",
        "            break\n",
        "        image = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "        image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]\n",
        "        cam_image = show_cam_on_image(image, grayscale_cam[i])\n",
        "        plt.imshow(cam_image)\n",
        "        plt.show()\n",
        "        count += 1\n",
        "    if count >= 5:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "K7bpOt42oJrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ishmatzaman/cse.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MaIe8VzoDWd",
        "outputId": "a52dadb2-97b8-4312-b8fb-394ab83a8908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cse'...\n",
            "remote: Enumerating objects: 537, done.\u001b[K\n",
            "remote: Counting objects: 100% (537/537), done.\u001b[K\n",
            "remote: Compressing objects: 100% (265/265), done.\u001b[K\n",
            "remote: Total 537 (delta 247), reused 526 (delta 239), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (537/537), 822.92 KiB | 9.04 MiB/s, done.\n",
            "Resolving deltas: 100% (247/247), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/cse\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryhVRnODpO0c",
        "outputId": "257ad815-b4e5-43dc-9486-2be14c7d84be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"Add new code from Colab\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkEOCy7lpOw1",
        "outputId": "3536f4c7-4a3e-4bcf-bbc7-73ac399c112e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@a22b6c642372.(none)')\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " git config --global user.email \"ismatzaman2222@gmail.com\"\n",
        "  git config --global user.name \"ishmatzaman\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "obOhbLqEpoCl",
        "outputId": "dfbd7120-1eb5-4678-bdd5-f7e17a073202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-110308633821>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-110308633821>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git config --global user.email \"ismatzaman2222@gmail.com\"\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}